{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cookie dialog found or already accepted\n",
      "No 'Save Login Info' dialog found\n",
      "No notifications dialog found\n",
      "Successfully logged in\n",
      "Starting continuous monitoring of insta_project_69\n",
      "Will check for new posts every 3 minutes\n",
      "Running initial scrape for insta_project_69...\n",
      "Found 5 posts to scrape\n",
      "Scraped post: DIR1lUoq4xh\n",
      "  Caption: Finally dekhte hai work hota hai ki nhi ðŸ’€ðŸŽ€ðŸ’¦ðŸ“ˆðŸ˜­ðŸ˜‚ðŸ˜­ðŸŽ‰ðŸ¤·ðŸ˜¡...\n",
      "Scraped post: DIRyc6tKDRi\n",
      "  Caption: Usee kisi aur sath dekhaa tab pataa challa... Moha...\n",
      "Scraped post: DIRySicqAXk\n",
      "  Caption: Abeyy yrrr valorant ko thoda pehle karne nhi huaaa...\n",
      "Scraped post: DIRyLygKnEv\n",
      "  Caption: I found a girlllll , beautiful and sweetttt aage k...\n",
      "Scraped post: DIRy0SYK4bo\n",
      "  Caption: Abbb pataa chalegaaa project asli mein kaam karta ...\n",
      "Initial scrape complete. Scraped 5 posts.\n",
      "\n",
      "[2025-04-11 02:01:53] Checking for new posts...\n",
      "No new posts found for insta_project_69\n",
      "Found 0 new posts\n",
      "Next check scheduled for: 2025-04-11 02:04:53\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "class InstagramMonitor:\n",
    "    def __init__(self, username, password):\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.options = Options()\n",
    "        self.options.add_argument('--headless')\n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "        self.options.add_argument('--disable-gpu')\n",
    "        self.options.add_argument('--window-size=1920x1080')\n",
    "        \n",
    "        self.driver = None\n",
    "        self.state_file = \"monitor_state.json\"\n",
    "        \n",
    "    def initialize_driver(self):\n",
    "        \"\"\"Initialize and return a new webdriver instance\"\"\"\n",
    "        self.driver = webdriver.Chrome(options=self.options)\n",
    "        return self.driver\n",
    "        \n",
    "    def login(self):\n",
    "        \"\"\"Log in to Instagram\"\"\"\n",
    "        self.driver.get(\"https://www.instagram.com/\")\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Accept cookies if the dialog appears\n",
    "        try:\n",
    "            cookie_button = WebDriverWait(self.driver, 5).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//button[contains(text(), 'Accept') or contains(text(), 'Allow')]\"))\n",
    "            )\n",
    "            cookie_button.click()\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            print(\"No cookie dialog found or already accepted\")\n",
    "        \n",
    "        try:\n",
    "            # Enter username\n",
    "            username_input = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.NAME, \"username\"))\n",
    "            )\n",
    "            username_input.send_keys(self.username)\n",
    "            \n",
    "            # Enter password\n",
    "            password_input = self.driver.find_element(By.NAME, \"password\")\n",
    "            password_input.send_keys(self.password)\n",
    "            \n",
    "            # Click login\n",
    "            login_button = self.driver.find_element(By.XPATH, \"//button[@type='submit']\")\n",
    "            login_button.click()\n",
    "            time.sleep(5)\n",
    "            \n",
    "            # Handle \"Save Login Info\" dialog\n",
    "            try:\n",
    "                not_now_button = WebDriverWait(self.driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Not Now') or contains(text(), 'Not now')]\"))\n",
    "                )\n",
    "                not_now_button.click()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                print(\"No 'Save Login Info' dialog found\")\n",
    "            \n",
    "            # Handle notifications dialog\n",
    "            try:\n",
    "                not_now_button = WebDriverWait(self.driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Not Now') or contains(text(), 'Not now')]\"))\n",
    "                )\n",
    "                not_now_button.click()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                print(\"No notifications dialog found\")\n",
    "                \n",
    "            print(\"Successfully logged in\")\n",
    "            return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Login failed: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def load_state(self, target_username):\n",
    "        \"\"\"Load the previous state to avoid duplicate scraping\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.state_file):\n",
    "                with open(self.state_file, \"r\") as f:\n",
    "                    state = json.load(f)\n",
    "                    if target_username in state:\n",
    "                        return state[target_username]\n",
    "            return {\"last_post_ids\": [], \"last_check\": None}\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading state: {str(e)}\")\n",
    "            return {\"last_post_ids\": [], \"last_check\": None}\n",
    "    \n",
    "    def save_state(self, target_username, state_data):\n",
    "        \"\"\"Save the current state to track what's been scraped\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.state_file):\n",
    "                with open(self.state_file, \"r\") as f:\n",
    "                    state = json.load(f)\n",
    "            else:\n",
    "                state = {}\n",
    "                \n",
    "            state[target_username] = state_data\n",
    "            \n",
    "            with open(self.state_file, \"w\") as f:\n",
    "                json.dump(state, f, indent=4)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving state: {str(e)}\")\n",
    "    \n",
    "    def get_latest_post_urls(self, target_username, max_posts=20):\n",
    "        \"\"\"Get the URLs of the latest posts\"\"\"\n",
    "        self.driver.get(f\"https://www.instagram.com/{target_username}/\")\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Scroll down a bit to load more posts\n",
    "        for _ in range(3):\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight/2);\")\n",
    "            time.sleep(2)\n",
    "        \n",
    "        # Collect post links (both normal posts and reels)\n",
    "        post_urls = []\n",
    "        \n",
    "        # Get regular posts links\n",
    "        post_elements = self.driver.find_elements(By.XPATH, \"//a[contains(@href, '/p/')]\")\n",
    "        for element in post_elements:\n",
    "            post_urls.append(element.get_attribute('href'))\n",
    "            \n",
    "        # Get reels links\n",
    "        reel_elements = self.driver.find_elements(By.XPATH, \"//a[contains(@href, '/reel/')]\")\n",
    "        for element in reel_elements:\n",
    "            post_urls.append(element.get_attribute('href'))\n",
    "        \n",
    "        # Remove duplicates and limit to max_posts\n",
    "        post_urls = list(set(post_urls))[:max_posts]\n",
    "        \n",
    "        return post_urls\n",
    "    \n",
    "    def get_post_details(self, post_url):\n",
    "        \"\"\"Get details for a specific post\"\"\"\n",
    "        # Extract post ID from URL\n",
    "        match = re.search(r'/(p|reel)/([^/]+)/', post_url)\n",
    "        if not match:\n",
    "            return None\n",
    "            \n",
    "        post_id = match.group(2)\n",
    "        post_type = match.group(1)  # 'p' for post, 'reel' for reel\n",
    "        \n",
    "        # Visit the post page\n",
    "        self.driver.get(post_url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        try:\n",
    "            # Try different selectors as Instagram's HTML structure changes frequently\n",
    "            caption_selectors = [\n",
    "                \"//div[contains(@class, '_a9zs')]/span\",  # Common caption container\n",
    "                \"//h1\",  # Sometimes captions are in h1\n",
    "                \"//div[contains(@class, '_a9zs')]\",  # Full caption container\n",
    "                \"//article//span[contains(text(), '')]\"  # Any text in the article\n",
    "            ]\n",
    "            \n",
    "            caption = \"\"\n",
    "            for selector in caption_selectors:\n",
    "                try:\n",
    "                    caption_elements = self.driver.find_elements(By.XPATH, selector)\n",
    "                    if caption_elements:\n",
    "                        caption = caption_elements[0].text\n",
    "                        if caption:\n",
    "                            break\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "            # Check for empty caption (might be just emojis)\n",
    "            if not caption:\n",
    "                # Try getting emojis or any content with a broader selector\n",
    "                try:\n",
    "                    emoji_elements = self.driver.find_elements(By.XPATH, \"//div[contains(@class, '_a9zs')]/*\")\n",
    "                    if emoji_elements:\n",
    "                        for el in emoji_elements:\n",
    "                            emoji_text = el.get_attribute('innerText') or el.get_attribute('textContent') or \"\"\n",
    "                            caption += emoji_text\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            if not caption:\n",
    "                caption = \"No caption found\"\n",
    "                \n",
    "            # Get timestamp if available\n",
    "            timestamp = \"\"\n",
    "            try:\n",
    "                time_element = self.driver.find_element(By.XPATH, \"//time\")\n",
    "                timestamp = time_element.get_attribute(\"datetime\")\n",
    "            except:\n",
    "                timestamp = datetime.now().isoformat()\n",
    "\n",
    "\n",
    "            image_url = \"\"\n",
    "            try:   \n",
    "               img_element = self.driver.find_element(By.XPATH, \"//article//img\")\n",
    "               image_url = img_element.get_attribute(\"src\")\n",
    "            except:\n",
    "               image_url = \"Image not found\"\n",
    "                \n",
    "            return {\n",
    "                \"post_id\": post_id,\n",
    "                \"type\": post_type,\n",
    "                \"url\": post_url,\n",
    "                \"caption\": caption,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"image_url\": image_url,\n",
    "                \"scraped_at\": datetime.now().isoformat()\n",
    "                \n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting post details for {post_url}: {str(e)}\")\n",
    "            return {\n",
    "                \"post_id\": post_id,\n",
    "                \"type\": post_type,\n",
    "                \"url\": post_url,\n",
    "                \"error\": str(e),\n",
    "                \"scraped_at\": datetime.now().isoformat()\n",
    "            }\n",
    "    \n",
    "    def posts_generator(self, target_username, max_posts=20, only_new=True):\n",
    "        \"\"\"Generator function to yield posts one by one\"\"\"\n",
    "        # Get current state to know what posts we've already scraped\n",
    "        state = self.load_state(target_username)\n",
    "        last_post_ids = set(state[\"last_post_ids\"])\n",
    "        \n",
    "        # Get all current post URLs\n",
    "        current_post_urls = self.get_latest_post_urls(target_username, max_posts)\n",
    "        \n",
    "        # Extract post IDs from URLs\n",
    "        current_post_ids = []\n",
    "        for url in current_post_urls:\n",
    "            match = re.search(r'/(p|reel)/([^/]+)/', url)\n",
    "            if match:\n",
    "                current_post_ids.append(match.group(2))\n",
    "        \n",
    "        # Determine which posts to scrape\n",
    "        if only_new:\n",
    "            # Only scrape posts we haven't seen before\n",
    "            posts_to_scrape = [(url, post_id) for url, post_id in zip(current_post_urls, current_post_ids) \n",
    "                             if post_id not in last_post_ids]\n",
    "        else:\n",
    "            # Scrape all posts regardless of whether we've seen them\n",
    "            posts_to_scrape = list(zip(current_post_urls, current_post_ids))\n",
    "        \n",
    "        if posts_to_scrape:\n",
    "            print(f\"Found {len(posts_to_scrape)} {'new ' if only_new else ''}posts to scrape\")\n",
    "            \n",
    "            # Update state with current posts for next run\n",
    "            state[\"last_post_ids\"] = current_post_ids\n",
    "            state[\"last_check\"] = datetime.now().isoformat()\n",
    "            self.save_state(target_username, state)\n",
    "            \n",
    "            # Yield each post's details\n",
    "            for post_url, post_id in posts_to_scrape:\n",
    "                post_details = self.get_post_details(post_url)\n",
    "                if post_details:\n",
    "                    yield post_details\n",
    "                time.sleep(2)  # Be gentle with the server\n",
    "        else:\n",
    "            print(f\"No {'new ' if only_new else ''}posts found for {target_username}\")\n",
    "            \n",
    "            # Still update the last check time\n",
    "            state[\"last_check\"] = datetime.now().isoformat()\n",
    "            self.save_state(target_username, state)\n",
    "            \n",
    "    def save_post_data(self, target_username, post):\n",
    "        \"\"\"Save the post data to files\"\"\"\n",
    "        # Ensure directory exists\n",
    "        os.makedirs(\"scraped_data\", exist_ok=True)\n",
    "        \n",
    "        # Append to JSON file\n",
    "        json_file = f\"scraped_data/{target_username}_posts.json\"\n",
    "        \n",
    "        if os.path.exists(json_file):\n",
    "            with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                try:\n",
    "                    posts = json.load(f)\n",
    "                except:\n",
    "                    posts = []\n",
    "        else:\n",
    "            posts = []\n",
    "        \n",
    "        posts.append(post)\n",
    "        \n",
    "        with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(posts, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        # Append to CSV file\n",
    "        csv_file = f\"scraped_data/{target_username}_posts.csv\"\n",
    "        file_exists = os.path.exists(csv_file)\n",
    "        \n",
    "        with open(csv_file, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "            fieldnames = [\"post_id\", \"type\", \"url\", \"caption\", \"timestamp\", \"image_url\", \"scraped_at\"]\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            \n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "            \n",
    "            # Filter to include only the fields defined in fieldnames\n",
    "            filtered_post = {key: post.get(key, \"\") for key in fieldnames}\n",
    "            writer.writerow(filtered_post)\n",
    "    \n",
    "    def run_initial_scrape(self, target_username, max_posts=20):\n",
    "        \"\"\"Run an initial scrape to establish a baseline\"\"\"\n",
    "        print(f\"Running initial scrape for {target_username}...\")\n",
    "        \n",
    "        post_count = 0\n",
    "        for post in self.posts_generator(target_username, max_posts, only_new=False):\n",
    "            print(f\"Scraped post: {post['post_id']}\")\n",
    "            print(f\"  Caption: {post['caption'][:50]}{'...' if len(post['caption']) > 50 else ''}\")\n",
    "            self.save_post_data(target_username, post)\n",
    "            post_count += 1\n",
    "            \n",
    "        print(f\"Initial scrape complete. Scraped {post_count} posts.\")\n",
    "    \n",
    "    def monitor_profile(self, target_username, interval_minutes=5, max_posts=20, run_initial=True):\n",
    "        \"\"\"Continuously monitor a profile for new posts\"\"\"\n",
    "        try:\n",
    "            self.initialize_driver()\n",
    "            \n",
    "            if not self.login():\n",
    "                print(\"Failed to login. Aborting monitoring.\")\n",
    "                self.driver.quit()\n",
    "                return\n",
    "            \n",
    "            print(f\"Starting continuous monitoring of {target_username}\")\n",
    "            print(f\"Will check for new posts every {interval_minutes} minutes\")\n",
    "            \n",
    "            # Run initial scrape if requested\n",
    "            if run_initial:\n",
    "                self.run_initial_scrape(target_username, max_posts)\n",
    "            \n",
    "            try:\n",
    "                while True:\n",
    "                    next_check_time = datetime.now().timestamp() + (interval_minutes * 60)\n",
    "                    next_check_str = datetime.fromtimestamp(next_check_time).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    \n",
    "                    print(f\"\\n[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Checking for new posts...\")\n",
    "                    \n",
    "                    post_count = 0\n",
    "                    for post in self.posts_generator(target_username, max_posts, only_new=True):\n",
    "                        print(f\"New post found: {post['post_id']}\")\n",
    "                        print(f\"  Caption: {post['caption'][:50]}{'...' if len(post['caption']) > 50 else ''}\")\n",
    "                        self.save_post_data(target_username, post)\n",
    "                        post_count += 1\n",
    "                    \n",
    "                    print(f\"Found {post_count} new posts\")\n",
    "                    print(f\"Next check scheduled for: {next_check_str}\")\n",
    "                    \n",
    "                    time.sleep(interval_minutes * 60)\n",
    "                    \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nMonitoring stopped by user\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in monitoring: {str(e)}\")\n",
    "            \n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Instagram credentials\n",
    "    INSTAGRAM_USERNAME = \"\"\n",
    "    INSTAGRAM_PASSWORD = \"\"\n",
    "    \n",
    "    # Target profile to monitor\n",
    "    TARGET_USERNAME = \"\"\n",
    "    \n",
    "    # Create the monitor\n",
    "    monitor = InstagramMonitor(INSTAGRAM_USERNAME, INSTAGRAM_PASSWORD)\n",
    "    \n",
    "    # Start monitoring\n",
    "    # Adjust these parameters as needed:\n",
    "    # - interval_minutes: How often to check for new posts (default: 5 minutes)\n",
    "    # - max_posts: Maximum number of posts to check each time (default: 20)\n",
    "    # - run_initial: Whether to do an initial scrape on startup (default: True)\n",
    "    monitor.monitor_profile(TARGET_USERNAME, interval_minutes=3, max_posts=10, run_initial=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
